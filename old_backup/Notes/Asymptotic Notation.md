#asymptotic #time-complexity #big-o 

When we talk about the *performance* of an algorithm, we do not mean the actual time in the measurable form it takes to execute the algorithm _(ie it takes 3 minutes to execute this algorithm)_. Because what time it takes to execute an algorithm depends on many factors. Like the user's computer specs or even the workload of the computer when the algorithm is running _(is there any background app running? is the user playing a video game?)_.

So, to measure the performance of an algorithm regardless of the hardware or software environment, asymptotic notation is used.  It basically defines how fast the program's runtime grows asymptotically. Basically, how much time a program/algorithm takes with a given input `n` where `n` grows towards infinity.

## See Also
1. [Asymptotic Notation - CS50](https://www.youtube.com/watch?v=iOq5kSKqeR4)